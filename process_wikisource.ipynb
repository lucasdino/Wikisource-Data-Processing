{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac083127-b928-45cf-a1b2-1b5b84fef26f",
   "metadata": {},
   "source": [
    "# Wikisource Dataset Processing\n",
    "---\n",
    "We'll be using the [English Wikisource Dataset](https://wikimedia.bringyour.com/enwikisource/20240320/?C=S&O=D) (Pages & Articles, 2.9GB compressed) to train a tokenizer and word embedding model. Unzipped, this dataset is 13GB which is too large for my laptop to work with efficiently. Additionally, the data comes unzipped as XML format and requires processing and cleaning before we can use it for our model.\n",
    "\n",
    "This workbook will let us visualize the data, define our text cleaning operation, and convert this larger dataset into Parquet chunks of a specified size (e.g., 50MB parquet files). We're using Parquet files because they allow for efficient data access and streaming -- which we'll use Pytorch Dataset and Dataloader objects to manipulate in training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052284d6-b221-4e3b-b9f1-d10d8978324e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Import Dependencies\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc27aecf-a332-4f5b-963c-b81ac3724be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adb507a-e265-4392-91ec-7274c11180f6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Understand Structure of Our Data\n",
    "---\n",
    "Because our data is stored in XML, we're using the 'ElementTree' module from Python's XML library. With this, we're able to efficiently load data from our XML file (don't need to load in the entire file - would cause OOM error if so).   \n",
    "We see that the specific data we want to extract is in the 'revision' tag and 'text' child tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "880429ee-e756-4782-996b-cd2f0bcba68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First - define the path to your data file (in our case, an XML)\n",
    "path = \"enwikisource-20240320-pages-articles.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa58610c-08c5-4fa2-be20-03f50fa046e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_xml_structure_modified(file_path, skip_elements=5000, max_elements=1000):\n",
    "    \"\"\"\n",
    "    Explore the XML structure by printing out the names of different element tags\n",
    "    and their counts, skipping the first 'skip_elements' and then considering\n",
    "    the next 'max_elements' within the file.\n",
    "\n",
    "    :param file_path: Path to the XML file.\n",
    "    :param skip_elements: Number of elements to skip before starting to count.\n",
    "    :param max_elements: Maximum number of elements to explore after skipping.\n",
    "    \"\"\"\n",
    "    tag_counts = defaultdict(int)\n",
    "    total_elements_processed = 0  # Total elements processed, including those skipped\n",
    "\n",
    "    for _, elem in ET.iterparse(file_path, events=(\"start\",)):\n",
    "        # Skip the first 'skip_elements'\n",
    "        if total_elements_processed < skip_elements:\n",
    "            total_elements_processed += 1\n",
    "            elem.clear()\n",
    "            continue  # Skip the rest of the loop and proceed to the next element\n",
    "        \n",
    "        # Start counting after skipping\n",
    "        tag_counts[elem.tag] += 1\n",
    "        total_elements_processed += 1\n",
    "        \n",
    "        # Break the loop if we have processed 'max_elements' after skipping\n",
    "        if total_elements_processed >= skip_elements + max_elements:\n",
    "            break\n",
    "        \n",
    "        elem.clear()  # Clear the element to save memory\n",
    "\n",
    "    # Print the discovered tags and their counts\n",
    "    for tag, count in tag_counts.items():\n",
    "        print(f\"Tag: {tag}, Count: {count}\")\n",
    "\n",
    "# Uncomment to run code\n",
    "# --------------------------------------------\n",
    "# explore_xml_structure_modified(path)\n",
    "\n",
    "        \n",
    "## --TAKEAWAY-- ## \n",
    "# From this, we can see the different tags that are in play. Printing out the whole element shows that what we're looking for is in the 'revision' tag, followed by the 'text' subclass. \n",
    "# From here, we'll need to further process our data so that we can pull just the Wikipedia text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423f12ef-de4a-4536-978c-5b85cca89763",
   "metadata": {},
   "source": [
    "## Define and Test Extraction and Processing Functions\n",
    "---\n",
    "Let's split this into a few steps.  \n",
    "1. **Random Samples**: Let's take a semi-random sample of 100 examples. We'll use this to test our Processing Function. (Note: To get a proper random sample, it would require us to loop through the whole dataset which is costly, so we'll take a semi-random sample of elements in the first ~100k articles).\n",
    "2. **Processing Function**: Let's write and test a processing function (optimized to run quickly) on our random samples.\n",
    "3. **Extraction Function**: Generator function that loops through our dataset and yields (same as 'return' but the function keeps running) lists of articles that contain - approximately - a defined size (in MB) of text. This will then be saved down as a parquet file.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324d3873-97d6-4bff-8d71-d804895f4962",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 1) Random Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb575be4-203f-4a64-80c4-75c5fa33bca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_sampler(path, num_samples):\n",
    "    \"\"\"\n",
    "        Function that iterates through our dataset. If a random number comes below a threshold and the article being currently processed is not None, it will append it to a sample list.\n",
    "        Returns said sample list.\n",
    "\n",
    "        Inputs:\n",
    "            path:        Filepath to dataset\n",
    "            num_samples: Length of list to be returned (number of samples sampled)\n",
    "\n",
    "        Returns list of len=num_samples of sampled articles\n",
    "    \"\"\"\n",
    "    threshold = 0.05\n",
    "    samples = []\n",
    "    \n",
    "    # Namespace dict to handle the XML namespace in tags\n",
    "    ns = {'mw': 'http://www.mediawiki.org/xml/export-0.10/'}\n",
    "\n",
    "    for event, elem in ET.iterparse(path, events=(\"end\",)):\n",
    "        if len(samples) >= num_samples:\n",
    "            break\n",
    "        if elem.tag == f\"{{{ns['mw']}}}page\":\n",
    "            for revision in elem.findall(f\"{{{ns['mw']}}}revision\"):\n",
    "                text_element = revision.find(f\"{{{ns['mw']}}}text\")\n",
    "                if text_element is not None:\n",
    "                    if random.random() < threshold:\n",
    "                        samples.append(text_element.text)\n",
    "            elem.clear()  # This frees up memory\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e290494-ebcb-4a41-a5d7-df3a57e8d614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate our random samples\n",
    "text_samples = rand_sampler(path, num_samples = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3faa70b4-63a8-4b18-b41a-7c3b52ae8a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Samples: 100\n",
      "------------------------------------------------------------\n",
      "{{other versions|Much Ado About Nothing (Shakespeare)}}\n",
      "{{header\n",
      " | title      = [[../]]\n",
      " | author     = William Shakespeare (1564-1616) | override_author = [[Author:William Shakespeare (1564-1616)|William Shakespeare]]\n",
      " | translator = \n",
      " | section    = Much adoe about Nothing\n",
      " | previous   = [[Shakespeare - First Folio facsimile (1910)/The Comedy of Errors|The Comedie of Errors]]\n",
      " | next       = [[Shakespeare - First Folio facsimile (1910)/Loves Labour's lost|Loues Labour's lost]]\n",
      " | notes      = \n",
      "}}\n",
      "{{AuxTOC|title=Acts|width=22em|\n",
      "*[[Shakespeare - First Folio facsimile (1910)/Much adoe about Nothing/Act 1|Act I]]\n",
      "*[[Shakespeare - First Folio facsimile (1910)/Much adoe about Nothing/Act 2|Act II]]\n",
      "*[[Shakespeare - First Folio facsimile (1910)/Much adoe about Nothing/Act 3|Act III]]\n",
      "*[[Shakespeare - First Folio facsimile (1910)/Much adoe about Nothing/Act 4|Act IV]]\n",
      "*[[Shakespeare - First Folio facsimile (1910)/Much adoe about Nothing/Act 5|Act V]]\n",
      "}}\n",
      "[[Category:Plays]]\n",
      "{{DEFAULTSORT:Much adoe about Nothing}}\n",
      "\n",
      "[[fr:Beaucoup de bruit pour rien]]\n",
      "[[Category:Plays]]\n"
     ]
    }
   ],
   "source": [
    "# Check our data\n",
    "print(f\"Num Samples: {len(text_samples)}\\n{'-'*60}\")\n",
    "print(text_samples[0])\n",
    "\n",
    "# We can see the structure. Some articles are short (we'll want to ignore these). There is also a 'header' at the start we want to remove. We'll experiment with our 'process_text' function\n",
    "# on this dataset until we get a function that cleans our text properly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80755b54-9a91-4aed-ba1e-5e757d034dcc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2) Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bd9bde3-56ef-4d5a-aa8e-e9ce2c6a1140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    # First, handle special cases by removing lines starting with *\n",
    "    text = '\\n'.join(line for line in text.split('\\n') if not line.strip().startswith('*'))\n",
    "    \n",
    "    cleaned_text = []\n",
    "    stack = []\n",
    "    for char in text:\n",
    "        if char in '{[<':\n",
    "            stack.append(char)\n",
    "        elif char in '}]>':\n",
    "            if stack:\n",
    "                last_open = stack.pop()\n",
    "                if (char == '}' and last_open != '{') or \\\n",
    "                   (char == ']' and last_open != '[') or \\\n",
    "                   (char == '>' and last_open != '<'):\n",
    "                    pass    # Handle syntax error or unbalanced brackets if needed\n",
    "        else:\n",
    "            if not stack:\n",
    "                cleaned_text.append(char)\n",
    "    \n",
    "    cleaned_text_str = ''.join(cleaned_text)\n",
    "    \n",
    "    # Removing content between '__' delimiters\n",
    "    while '__' in cleaned_text_str:\n",
    "        open_index = cleaned_text_str.find('__')\n",
    "        close_index = cleaned_text_str.find('__', open_index + 2)\n",
    "        if close_index == -1:\n",
    "            break\n",
    "        cleaned_text_str = cleaned_text_str[:open_index] + cleaned_text_str[close_index + 2:]\n",
    "        \n",
    "    return cleaned_text_str\n",
    "\n",
    "\n",
    "def clean_processed_text(text):\n",
    "    # Replace custom heading formats with HTML tags\n",
    "    text = re.sub(r'====(.*?)====', r'<h4>\\1</h4>', text)      # Replace '====Heading 4====' with '<h4>Heading 4</h4>'\n",
    "    text = re.sub(r'===(.*?)===', r'<h3>\\1</h3>', text)        # Replace '===Heading 3===' with '<h3>Heading 3</h3>'\n",
    "    text = re.sub(r'==(.*?)==', r'<h2>\\1</h2>', text)          # Replace '==Heading 2==' with '<h2>Heading 2</h2>'\n",
    "    text = re.sub(r'=(.*?)=', r'<h1>\\1</h1>', text)            # Replace '=Heading 1=' with '<h1>Heading 1</h1>'\n",
    "    \n",
    "    text = text.strip('\\n ')                                   # Strip leading and trailing newline characters and spaces\n",
    "    text = [line.lstrip(': ') for line in text.split('\\n') ]   # Remove leading colons and spaces from each line\n",
    "    text = '\\n'.join(text)                                     # Rejoin cleaned lines\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)                     # Replace three or more consecutive newlines with two newlines\n",
    "    return text\n",
    "\n",
    "def process_text(text):\n",
    "    \"\"\" \n",
    "        Parent function - pass in a string and returns your cleaned_text and a boolean that can be used to discard short passages\n",
    "        *Note* If you set the threshold for passage length to be 700, you discard ~1/3 of the passages\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        return \"\", False\n",
    "    preprocessed_text = text_preprocessing(text)\n",
    "    cleaned_text = clean_processed_text(preprocessed_text)\n",
    "    if len(cleaned_text) > 700:\n",
    "        return cleaned_text, True\n",
    "    else:\n",
    "        return cleaned_text, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88a460bf-d41b-4d4b-ad0b-9136472f4a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples remaining: 59/100\n",
      "Printing: cleaned_samples[24]:\n",
      "------------------------------------------------------------\n",
      "<h1>Character</h1>\n",
      "\n",
      "'', , ''\n",
      "\n",
      "It is said of the ermine that it will suffer capture rather than allow pollution to touch its glossy coat, but take away that coat and the animal is worthless.\n",
      "\n",
      "We have ermines in higher life&mdash;those who love display.  The desire to seem, rather than to be, is one of the faults which our age, as well as other ages, must deplore.\n",
      "\n",
      "Appearance too often takes the place of reality&mdash;the stamp of the coin is there, and the glitter of the gold, but, after all, it is but a worthless wash.  Sham is carried into every department of life, and we are being corrupted by show and surface.  We are too apt to judge people by what they have, rather than by what they are; we have too few Hamlets who are bold enough to proclaim, \"I know not seem!\"\n",
      "\n",
      "The counterfeit, however, only proves the value of the coin, and, although reputation may in some degree be taking the place of character, yet the latter has lost none of its worth, and, now, as of old, is a priceless gem, wherever found.  Its absence and presence, alike, prove its value.  Have you not conversed with those whose brilliant wit, pungent sarcasm and well-framed sentences failed to conceal a certain indescribable something which made you distrust every word they uttered?  Have you not listened to those whose eloquence dazzled, whose pretended earnestness enkindled in you an enthusiasm equal to their own, and yet, have you not felt that behind all this there was lurking a monster that repelled the admiration which their genius attracted?  Are there not those, whom like the Greeks we fear, even when they are bringing gifts?  That something is want of character, or, to speak more truly, the possession of bad character, and it shows itself alike in nations and individuals.\n",
      "\n",
      "Eschines was talented; his oration against the crowning of Demosthenes was a masterly production, excellently arranged, elegantly written and effectively delivered, so extraordinary was its merits, that, when he afterward, as an exile, delivered it before a Roadian audience, they expressed their astonishment that it had not won for him his cause, but it fell like a chilling blast upon his hearers at Athens, because he was the \"hireling of Philip\".\n",
      "\n",
      "Napoleon swept like a destroying angel over almost the entire eastern world, evincing a military genius unsurpassed, skill marvelous in its perfection, and a courage which savored almost of rashness, yet ever demonstrated the wisdom of its dictates.  For a while he seemed to have robbed fortune of her secret, and bewildered nations gazed in silence while he turned the streams of success according to his vascillating whims.\n",
      "\n",
      "Although endowed with a perception keen enough to discern the hidden plans of opposing generals, he could but see one road to immortality&mdash;a path which led through battle-fields and marshes wet with human gore; over rivers of blood and streams of tears that flowed from orphans' eyes&mdash;a path along whose length the widow's wail made music for his marching hosts.  But he is fallen, and over his tomb no mourner weeps.  Talent, genius, power, these he had&mdash;character, he had none.\n",
      "\n",
      "But there are those who have both influence through life and unending praises after death; there are those who have by their ability, inspired the admiration of the people and held it by the purity of their character.  It is often remarked that some men have a name greater than their works will justify; the secret lies in the men themselves.\n",
      "\n",
      "It was his well-known character, not less than his eloquent words; his deep convictions, not less than the fire of his utterance; his own patriotism, not less than his invectives against the Macedonian that brought to the lips of the reanimated Greeks that memorable sentence, \"Let us go against Philip.\"\n",
      "\n",
      "Perhaps we could not find better illustrations of the power and worth of character than are presented in the lives of two of our own countrymen&mdash;names about which cluster in most sacred nearness the affections of the American people&mdash;honored dust over which have fallen the truest tears of sorrow ever shed by a nation for its heroes&mdash;the father and savior of their common country&mdash;the one, the appointed guardian of its birth; the other, the preserver of its life.\n",
      "\n",
      "Both were reared by the hand of Providence for the work entrusted to their care, both were led by nature along the rugged path of poverty; both formed a character whose foundations were laid broad and deep in the purest truths of morality&mdash;a character which stood unshaken amid the terrors of war and the tranquillity of peace; a character which allowed neither cowardice upon the battle-field nor tyranny in the presidential chair.  Thus did they win the hearts of their countrymen and prepare for themselves a lasting place of rest in the tender memories of a grateful people.\n",
      "\n",
      "History but voices our own experience when it awards to true nobility of character the highest place among the enviable possessions of man.  Nor is it the gift of fortune.  In this, at least, we are not creatures of circumstances; talent, special genius may be the gift of nature; position in society the gift of birth; respect may be bought with wealth; but neither one nor all of these can give character.  It is a slow but sure growth to which every thought and action lends its aid.  To form character is to form grooves in which are to flow the purposes of our lives.  It is to adopt principles which are to be the measure of our actions, the criteria of our deeds.  This we are doing each day, either consciously or unconsciously.  There is character formed by our association with each friend, by every aspiration of the heart, by every object toward which our affections go out, yea, by every thought that flies on its lightning wing through the dark recesses of the brain.\n",
      "\n",
      "It is a law of mind that it acts most readily in familiar paths, hence, repetition forms habit, and almost before we are aware, we are chained to a certain routine of action from which it is difficult to free ourselves.  We imitate that which we admire.  If we revel in stories of blood, and are pleased with the sight of barbaric cruelty, we find it easy to become a Caligula or a Domitian; we picture to ourselves scenes of cruelty in which we are actors, and soon await only the opportunity to vie in atrocity with the Neroes of the past.\n",
      "\n",
      "If we delight in gossip, and are not content unless each neighbor is laid upon the dissecting table, we form a character unenviable indeed, and must be willing to bear the contempt of all the truly good, while we roll our bit of scandal as a sweet morsel under the tongue.\n",
      "\n",
      "But if each day we gather some new truths, plant ourselves more firmly upon principles which are eternal, guard every thought and action, that it may be pure, and conform our lives more nearly to that Perfect Model, we shall form a character that will be a fit background on which to paint the noblest deeds and the grandest intellectual and moral achievements; a character that cannot be concealed, but which will bring success in this life and form the best preparation for that which is beyond.\n",
      "\n",
      "The formation of character is a work which continues through life, but at no time is it so active as in youth and early manhood.  At this time impressions are most easily made, and mistakes most easily corrected.  It is the season for the sowing of the seed&mdash;the springtime of life.  There is no complaint in the natural world because each fruit and herb brings forth after its kind; there is no complaint if a neglected seed-time brings a harvest of want; there is no cry of injustice if thistles spring from thistle-seed sown.  As little reason have we to murmur if in after-life we discover a character dwarfed and deformed by the evil thoughts and actions of today; as little reason have we to impeach the wisdom of God if our wild oats, as they are called in palliation, leave scars upon our manhood, which years of reform fail to wear away.\n",
      "\n",
      "Character is the entity, the individuality of the person, shining from every window of the soul, either as a beam of purity, or as a clouded ray that betrays the impurity within.  The contest between light and darkness, right and wrong, goes on; day by day, hour by hour, moment by moment, our characters are being formed, and this is the all-important question which comes to us in accents ever growing fainter as we journey from the cradle to the grave, \"Shall those characters be good or bad?\"\n"
     ]
    }
   ],
   "source": [
    "# Function to clean our samples and print out a random cleaned sample from the list\n",
    "cleaned_samples = []\n",
    "for text in text_samples:\n",
    "    cleaned_text, append_text = process_text(text)\n",
    "    if append_text:\n",
    "        cleaned_samples.append(cleaned_text)\n",
    "\n",
    "print(f\"Samples remaining: {len(cleaned_samples)}/{len(text_samples)}\")\n",
    "rand_idx = random.randrange(0, len(cleaned_samples))\n",
    "print(f\"Printing: cleaned_samples[{rand_idx}]:\\n{'-'*60}\")\n",
    "print(random.choice(cleaned_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4543dbd0-fcd4-435a-9b48-75e4237f4121",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3) Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "935695c7-b41a-4c14-98d8-5865f60af7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_clean_texts(file_path, start_num=0, batch_size_MB=50, testing_fn=True):\n",
    "    \"\"\"\n",
    "    A generator that iterates through an XML file and yields batches of cleaned text strings.\n",
    "\n",
    "    Input: \n",
    "        file_path:  Path to XML file.\n",
    "        start_num:  Specify which passage to start at (i.e., discards first 'start_num' passages).\n",
    "        batch_size: Appx. size per batch in MB.\n",
    "\n",
    "    Yields batches of strings of length 'batch_size' until the end of the file is reached.\n",
    "    \"\"\"\n",
    "    chars_per_batch = batch_size_MB*(1024**2) / 0.6     # From quick empirical analysis - imperfect but close\n",
    "    chars_processed = 0\n",
    "    pages_processed = 0\n",
    "    extracted_texts = []\n",
    "\n",
    "    # Namespace dict to handle the XML namespace in tags\n",
    "    ns = {'mw': 'http://www.mediawiki.org/xml/export-0.10/'}\n",
    "\n",
    "    for event, elem in ET.iterparse(file_path, events=(\"end\",)):\n",
    "        if testing_fn:\n",
    "            if pages_processed == 10000:\n",
    "                break      # Ensures you don't iterate over the whole dataset\n",
    "        if elem.tag == f\"{{{ns['mw']}}}page\":\n",
    "            pages_processed += 1\n",
    "\n",
    "            if pages_processed > start_num:\n",
    "                for revision in elem.findall(f\"{{{ns['mw']}}}revision\"):\n",
    "                    text_element = revision.find(f\"{{{ns['mw']}}}text\")\n",
    "                    if text_element is not None:\n",
    "                        clean_text, append_text = process_text(text_element.text)  # Assuming process_text returns a tuple\n",
    "                        if append_text:\n",
    "                            extracted_texts.append(clean_text)\n",
    "                            chars_processed += len(clean_text)\n",
    "                            if chars_processed >= chars_per_batch:\n",
    "                                chars_processed = 0\n",
    "                                yield extracted_texts\n",
    "                                extracted_texts = []  # Reset for the next batch\n",
    "                \n",
    "            elem.clear()  # This helps in freeing up memory\n",
    "    \n",
    "    # Yield any remaining texts if they don't fill up a complete batch\n",
    "    if extracted_texts:\n",
    "        yield extracted_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8de50b02-4189-438d-8fe6-d17e063c6797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_chunk_as_parquet(chunk, batch_number):\n",
    "    \"\"\" \n",
    "        Function that takes a list of cleaned texts and saves it as a Parquet file\n",
    "    \"\"\"\n",
    "    directory = 'chunked_data'\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    filename = f'{directory}/enwiki_20240320_{batch_number}.parquet'\n",
    "    structured_chunk = [[text] for text in chunk]  # Convert each string to a list\n",
    "    chunk_df = pd.DataFrame(structured_chunk, columns=['text'])\n",
    "    chunk_df.to_parquet(filename)\n",
    "    print(f'Saved {filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37abd427-3160-4e18-be5d-18095a6bf0d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 has 779 articles in it.\n",
      "Batch 2 has 831 articles in it.\n",
      "Saved chunked_data/enwiki_20240320_testbatch.parquet\n",
      "Batch 3 has 545 articles in it.\n",
      "Batch 4 has 821 articles in it.\n",
      "Batch 5 has 1193 articles in it.\n",
      "Batch 6 has 558 articles in it.\n"
     ]
    }
   ],
   "source": [
    "# Run to test if our code is working properly\n",
    "# for i, clean_list in enumerate(extract_clean_texts(path, start_num=0, batch_size_MB=10, testing_fn=True)):\n",
    "#     print(f\"Batch {i+1} has {len(clean_list)} articles in it.\")\n",
    "#     if i == 1:\n",
    "#         save_chunk_as_parquet(clean_list, \"testbatch\")    # Test export of our parquet file "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4100d84d-02a3-44d9-9823-acc7cdfc6be0",
   "metadata": {},
   "source": [
    "## Export and Process our Full Dataset\n",
    "---\n",
    "Now that we know our functions are working, processing and saving down our full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3681eb6f-ced8-4871-b984-14d08471e9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunked_data/enwiki_20240320_1.parquet\n",
      "Saved chunked_data/enwiki_20240320_2.parquet\n",
      "Saved chunked_data/enwiki_20240320_3.parquet\n",
      "Saved chunked_data/enwiki_20240320_4.parquet\n",
      "Saved chunked_data/enwiki_20240320_5.parquet\n",
      "Saved chunked_data/enwiki_20240320_6.parquet\n",
      "Saved chunked_data/enwiki_20240320_7.parquet\n",
      "Saved chunked_data/enwiki_20240320_8.parquet\n",
      "Saved chunked_data/enwiki_20240320_9.parquet\n",
      "Saved chunked_data/enwiki_20240320_10.parquet\n",
      "Saved chunked_data/enwiki_20240320_11.parquet\n",
      "Saved chunked_data/enwiki_20240320_12.parquet\n",
      "Saved chunked_data/enwiki_20240320_13.parquet\n",
      "Saved chunked_data/enwiki_20240320_14.parquet\n",
      "Saved chunked_data/enwiki_20240320_15.parquet\n",
      "Saved chunked_data/enwiki_20240320_16.parquet\n",
      "Saved chunked_data/enwiki_20240320_17.parquet\n",
      "Saved chunked_data/enwiki_20240320_18.parquet\n",
      "Saved chunked_data/enwiki_20240320_19.parquet\n",
      "Saved chunked_data/enwiki_20240320_20.parquet\n",
      "Saved chunked_data/enwiki_20240320_21.parquet\n",
      "Saved chunked_data/enwiki_20240320_22.parquet\n",
      "Saved chunked_data/enwiki_20240320_23.parquet\n",
      "Saved chunked_data/enwiki_20240320_24.parquet\n",
      "Saved chunked_data/enwiki_20240320_25.parquet\n",
      "Saved chunked_data/enwiki_20240320_26.parquet\n",
      "Saved chunked_data/enwiki_20240320_27.parquet\n",
      "Saved chunked_data/enwiki_20240320_28.parquet\n",
      "Saved chunked_data/enwiki_20240320_29.parquet\n",
      "Saved chunked_data/enwiki_20240320_30.parquet\n",
      "Saved chunked_data/enwiki_20240320_31.parquet\n",
      "Saved chunked_data/enwiki_20240320_32.parquet\n",
      "Saved chunked_data/enwiki_20240320_33.parquet\n",
      "Saved chunked_data/enwiki_20240320_34.parquet\n",
      "Saved chunked_data/enwiki_20240320_35.parquet\n",
      "Saved chunked_data/enwiki_20240320_36.parquet\n",
      "Saved chunked_data/enwiki_20240320_37.parquet\n",
      "Saved chunked_data/enwiki_20240320_38.parquet\n",
      "Saved chunked_data/enwiki_20240320_39.parquet\n",
      "Saved chunked_data/enwiki_20240320_40.parquet\n",
      "Saved chunked_data/enwiki_20240320_41.parquet\n",
      "Saved chunked_data/enwiki_20240320_42.parquet\n",
      "Saved chunked_data/enwiki_20240320_43.parquet\n",
      "Saved chunked_data/enwiki_20240320_44.parquet\n",
      "Saved chunked_data/enwiki_20240320_45.parquet\n",
      "Saved chunked_data/enwiki_20240320_46.parquet\n",
      "Saved chunked_data/enwiki_20240320_47.parquet\n",
      "Saved chunked_data/enwiki_20240320_48.parquet\n",
      "Saved chunked_data/enwiki_20240320_49.parquet\n",
      "Saved chunked_data/enwiki_20240320_50.parquet\n",
      "Saved chunked_data/enwiki_20240320_51.parquet\n",
      "Saved chunked_data/enwiki_20240320_52.parquet\n",
      "Saved chunked_data/enwiki_20240320_53.parquet\n",
      "Saved chunked_data/enwiki_20240320_54.parquet\n",
      "Saved chunked_data/enwiki_20240320_55.parquet\n",
      "Saved chunked_data/enwiki_20240320_56.parquet\n",
      "Saved chunked_data/enwiki_20240320_57.parquet\n",
      "Saved chunked_data/enwiki_20240320_58.parquet\n",
      "Saved chunked_data/enwiki_20240320_59.parquet\n",
      "Saved chunked_data/enwiki_20240320_60.parquet\n",
      "Saved chunked_data/enwiki_20240320_61.parquet\n",
      "Saved chunked_data/enwiki_20240320_62.parquet\n",
      "Saved chunked_data/enwiki_20240320_63.parquet\n",
      "Saved chunked_data/enwiki_20240320_64.parquet\n",
      "Saved chunked_data/enwiki_20240320_65.parquet\n",
      "Saved chunked_data/enwiki_20240320_66.parquet\n",
      "Saved chunked_data/enwiki_20240320_67.parquet\n",
      "Saved chunked_data/enwiki_20240320_68.parquet\n",
      "Saved chunked_data/enwiki_20240320_69.parquet\n",
      "Saved chunked_data/enwiki_20240320_70.parquet\n",
      "Saved chunked_data/enwiki_20240320_71.parquet\n",
      "Saved chunked_data/enwiki_20240320_72.parquet\n"
     ]
    }
   ],
   "source": [
    "# Uncomment and run when ready to clean full dataset\n",
    "for i, chunk in enumerate(extract_clean_texts(path, start_num=0, batch_size_MB=50, testing_fn=False)):\n",
    "    save_chunk_as_parquet(chunk, i+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ccc9df-b39e-48d5-96e6-dd733f382ccf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## View and Analyze Parquet Files\n",
    "---\n",
    "Quick code to open random Parquet file and view the contents to see if code is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4288237e-0801-4f0c-a46b-7ffa259ecb2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 831 entries, 0 to 830\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    831 non-null    object\n",
      "dtypes: object(1)\n",
      "memory usage: 6.6+ KB\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "TO what a cumbersome unwieldiness\n",
      "And burdenous corpulence my love had grown,\n",
      "But that I did, to make it less,\n",
      "And keep it in proportion,\n",
      "Give it a diet, made it feed upon\n",
      "That which love worst endures, discretion\n",
      "\n",
      "Above one sigh a day I allow'd him not,\n",
      "Of which my fortune, and my faults had part;\n",
      "And if sometimes by stealth he got\n",
      "A she sigh from my mistress' heart,\n",
      "And thought to feast upon that, I let him see\n",
      "'Twas neither very sound, nor meant to me.\n",
      "\n",
      "If he wrung from me a tear, I brined it so\n",
      "With scorn and shame, that him it nourish'd not;\n",
      "If he suck'd hers, I let him know\n",
      "'Twas not a tear which he had got;\n",
      "His drink was counterfeit, as was his meat;\n",
      "For eyes, which roll towards all, weep not, but sweat.\n",
      "\n",
      "Whatever he would dictate I writ that,\n",
      "But burnt her letters when she writ to me;\n",
      "And if that favour made him fat,\n",
      "I said, \"If any title be\n",
      "Convey'd by this, ah! what doth it avail,\n",
      "To be the fortieth name in an entail?\"\n",
      "\n",
      "Thus I reclaim'd my buzzard love, to fly\n",
      "At what, and when, and how, and where I choose.\n",
      "Now negligent of sports I lie,\n",
      "And now, as other falconers use,\n",
      "I spring a mistress, swear, write, sigh, and weep;\n",
      "And the game kill'd, or lost, go talk or sleep.\n"
     ]
    }
   ],
   "source": [
    "directory = 'chunked_data'\n",
    "parquet_files = [f for f in os.listdir(directory) if f.endswith('.parquet')]\n",
    "\n",
    "if parquet_files:\n",
    "    random_file = random.choice(parquet_files)\n",
    "    # file_path = os.path.join(directory, \"enwiki_20240320_1.parquet\")\n",
    "    file_path = os.path.join(directory, random_file)\n",
    "    df = pd.read_parquet(file_path)\n",
    "    \n",
    "    df.info()\n",
    "    print(f\"\\n{'-'*100}\\n\")\n",
    "    random_row = df.sample(n=1).iloc[0]\n",
    "    print(random_row['text']) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
